#! /bin/bash

#---------------------------------------------------------------------------OSD Setup Script---------------------------------------------------------

cd "$(dirname "$0")"

#We source the export file to include the $cluster_name varialbe in our script
source export_file

############These are only the step by step with there output---script will have to be adapted after

#We find and create a table with the available disks (/dev/sda is already used for the OS)

echo $cluster_name
disk_partition=( $(fdisk -l 2>/dev/null | grep "/dev/sd[b-z][^1-9]" | cut -d" " -f2 | cut -d":" -f1 | sort) )
#/dev/sdb
#/dev/sdc
echo  ${#disk_partition[@]} 
echo  ${disk_partition[@]} 
#Create partitions on the disks
z=0
while [ $z -lt ${#disk_partition[@]} ] 
do
#We do a loop in order to run the fdisk command and all the requiered disks

        echo "We are going to configure ${disk_partition[z]}"
        fdisk ${disk_partition[z]} 2> /dev/null
        (( z++ ))
done


y=0
while [ $y -lt ${#disk_partition[@]} ]
do
#We do a loop until we have created as much OSDs are we have disks

	#1) #Create our first OSD
	osd_uuid=$(uuidgen)

	#Create the OSD
	osd_num=$(ceph --cluster $cluster_name osd create $osd_uuid)  #for it to have a value of 0
	#This will return the OSD number starting from 0 and add the value inside a variable


	#Create the OSD directory
	mkdir /var/lib/ceph/osd/$cluster_name-$osd_num

	first_partition=( $(fdisk -l 2>/dev/null | cut -d" " -f1 | grep "^/dev/sd[b-z][1]" | sort) )
	#This will output only the partitions created earlier
	#/dev/sdb1
	#/dev/sdc1

	mkfs -t xfs ${first_partition[y]} #This will represent /dev/sdb1, /dev/sdc1, etc

	mount -o inode64 ${first_partition[y]} /var/lib/ceph/osd/$cluster_name-$osd_num

	#Initialize the OSD data directory
	ceph-osd --cluster $cluster_name -i $osd_num --mkfs --mkkey --osd-uuid $osd_uuid

	#Register the OSD Authentification key
	ceph --cluster $cluster_name auth add osd."$osd_num" osd 'allow *' mon 'allow profile osd' -i /var/lib/ceph/osd/$cluster_name-$osd_num/keyring

	#Add your Ceph Node to the CRUSH map.
	ceph --cluster $cluster_name osd crush add-bucket $HOSTNAME host
	
	#Place the Ceph Node under the root default
	ceph --cluster $cluster_name osd crush move $HOSTNAME root=default

	#Add the OSD to the CRUSH map so that it can begin receiving data. You may also decompile the CRUSH map, add the OSD to the device list, add the host as a bucket (if itâ€™s not already in the CRUSH map), add the device as an item in the host, assign it a weight, recompile it and set it.
	ceph --cluster $cluster_name  osd crush add osd."$osd_num" 1.0 host="$HOSTNAME"

	#Start the OSD daemon
	ceph-osd -i $osd_num --cluster $cluster_name


	#Add the mount to /etc/fstab
	echo "${first_partition[y]}       /var/lib/ceph/osd/$cluster_name-$osd_num        xfs     rw,inode64      0       2" >> /etc/fstab
	
	#RC LOCAL
	sed_line=13
	#upstart="sudo start ceph-mon "id=$HOSTNAME" "cluster=$cluster_name""
	upstart="ceph-osd -i $osd_num --cluster $cluster_name"
	sudo sed -i "$sed_line a $upstart" /etc/rc.local
	(( sed_line++ ))

	(( y++ ))
done

ceph --cluster $cluster_name osd tree
ceph --cluster $cluster_name -s

echo -e "\nOSD Setup has been completed\n"
